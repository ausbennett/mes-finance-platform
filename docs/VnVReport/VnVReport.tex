\documentclass[12pt, titlepage]{article}

\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{hyperref}
\hypersetup{
    colorlinks,
    citecolor=black,
    filecolor=black,
    linkcolor=red,
    urlcolor=blue
}
\usepackage[round]{natbib}

\input{../Comments}
\input{../Common}

\begin{document}

\title{Verification and Validation Report: \progname} 
\author{\authname}
\date{\today}
	
\maketitle

\pagenumbering{roman}

\section{Revision History}

\begin{tabularx}{\textwidth}{p{3cm}p{2cm}X}
\toprule {\bf Date} & {\bf Version} & {\bf Notes}\\
\midrule
Date 1 & 1.0 & Notes\\
Date 2 & 1.1 & Notes\\
\bottomrule
\end{tabularx}

~\newpage

\section{Symbols, Abbreviations and Acronyms}

\renewcommand{\arraystretch}{1.2}
\begin{tabular}{l l} 
  \toprule		
  \textbf{symbol} & \textbf{description}\\
  \midrule 
  T & Test\\
  \bottomrule
\end{tabular}\\

\wss{symbols, abbreviations or acronyms -- you can reference the SRS tables if needed}

\newpage

\tableofcontents

\listoftables %if appropriate

\listoffigures %if appropriate

\newpage

\pagenumbering{arabic}

This document ...

\section{Functional Requirements Evaluation}

\section{Nonfunctional Requirements Evaluation}

\subsection{Usability}
		
\subsection{Performance}

\subsection{etc.}
	
\section{Comparison to Existing Implementation}	

This section will not be appropriate for every project.

\section{Unit Testing}

Most files, excluding the 3rd-party library, top-level and GUI modules had corresponding unit tests.
All tests were performed using jest for testing javascript, and each commit to the main branch must pass all the tests.
All the tests passed as seen in the Test Report \ref{test_report}.

\section{Changes Due to Testing}

\wss{This section should highlight how feedback from the users and from 
the supervisor (when one exists) shaped the final product.  In particular 
the feedback from the Rev 0 demo to the supervisor (or to potential users) 
should be highlighted.}

\section{Automated Testing}

The tests were set up to automatically run in GitHub Actions whenever a commit was pushed to the $\mathtt{main}$ branch.
The configuration for the CI/CD automation can be found at \url{}
and the test pipeline can be viewed at \url{}.
		
\section{Trace to Requirements}

\begin{table}[h!]
\begin{tabularx}{\textwidth}{p{5cm}p{5cm}}
\toprule {\bf Test Suite} & {\bf Functional Requirement}\\
\midrule 
emailer.test.js & FR1.5, FR3.2\\
reconciler.test.js & FR1.1, FR1.4, FR1.6, FR1.9, INR1, SPLR1-3\\
requests.test.js & FR1.2, FR1.3, FR1.8, PVR1\\ 
Account management test suite (TBD) & FR1.7, FR3.1, FR3.2\\
Usability testing (via checklist) & APR1, APR2, STYR1, STYR2, EUR1-3, PIR1, LER2, UAPR1, ACSR1, SCR1-5, CLTR1\\
Static analysis (via checklist) & POAR1, LOR1, WR1, RLR1, ACSR2, MR2, SR1, LR1, LR2, STR1\\
Load testing & CPR1-3, SER2\\
\bottomrule
\end{tabularx}
\caption{Mapping between test suites and requirements}
\end{table}

\section{Trace to Modules}

\begin{table}[h!]
\begin{tabularx}{\textwidth}{p{5cm}p{5cm}}
\toprule {\bf Test Suite} & {\bf Module}\\
\midrule 
emailer.test.js & Notification Module (M4), Authentication Module (M6), Emailer API (M7)\\
reconciler.test.js & Clubs, User, Request Database(M10-12)\\
requests.test.js & Requests Module(M3), Requests Controller (M9)\\
Account management test suite (TBD) & Account Management Module (M2), User Dashboard Module (M5), Account Management Controller (M8)\\
Usability testing (via checklist) & Graphical User Interface (M13)\\
\bottomrule
\end{tabularx}
\caption{Mapping between test suites and modules}
\end{table}

\section{Code Coverage Metrics}

\section{Appendix}
\subsection{Test Report} \label{test_report}
\begin{small} 
  \begin{verbatim} 
     PASS  tests/requests.test.js
      Request Endpoints and Model Validations
        Model Validations
          should require necessary reimbursement fields (3 ms)
        Reimbursement Endpoints
          should create reimbursement with file upload (34 ms)

    Test Suites: 1 passed, 1 total
    Tests:       2 passed, 2 total
    Snapshots:   0 total
    Time:        0.927 s, estimated 1 s
    Ran all test suites.
  \end{verbatim}
\end{small}

\bibliographystyle{plainnat}
\bibliography{../../refs/References}

\newpage{}
\section*{Appendix --- Reflection}

The information in this section will be used to evaluate the team members on the
graduate attribute of Reflection.

\input{../Reflection.tex}

\begin{enumerate}
  \item What went well while writing this deliverable? 
  \item What pain points did you experience during this deliverable, and how
    did you resolve them?
  \item Which parts of this document stemmed from speaking to your client(s) or
  a proxy (e.g. your peers)? Which ones were not, and why?
  \item In what ways was the Verification and Validation (VnV) Plan different
  from the activities that were actually conducted for VnV?  If there were
  differences, what changes required the modification in the plan?  Why did
  these changes occur?  Would you be able to anticipate these changes in future
  projects?  If there weren't any differences, how was your team able to clearly
  predict a feasible amount of effort and the right tasks needed to build the
  evidence that demonstrates the required quality?  (It is expected that most
  teams will have had to deviate from their original VnV Plan.)
\end{enumerate}

\end{document}
